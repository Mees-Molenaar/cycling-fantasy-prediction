{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo2KS-VZTWNI"
   },
   "source": [
    "# Predict the profile icon\n",
    "\n",
    "The model trained for the profile prediction is a ResNet 18 deep layer, which is trained in the profile_icon_classification notebook. We predict profile icons using the Pytorch library to fill in missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJyTQnu_TV5-"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l6nN0ZgTTLQr"
   },
   "outputs": [],
   "source": [
    "# And set the device for GPU usage\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJTKITMQTsfY"
   },
   "outputs": [],
   "source": [
    "# Set seeding\n",
    "torch.manual_seed(420)\n",
    "np.random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "7K_MIOpsTuPB",
    "outputId": "553f1a62-2433-48b5-90e3-b80077cbd6a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at content/\n"
     ]
    }
   ],
   "source": [
    "# Connect google drive to notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "drive.mount('content/', force_remount=True)\n",
    "base = Path('/content/content/My Drive/TdF/Profiles')\n",
    "sys.path.append(str(base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9qYIMcBTwLh"
   },
   "outputs": [],
   "source": [
    "# The classes we have\n",
    "classes = ['p1', 'p2', 'p3', 'p4', 'p5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eR9VeRrqmQdE"
   },
   "source": [
    "## Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hey3SaVgmSZs"
   },
   "outputs": [],
   "source": [
    "def reverse_normalization(tensor_in, mean, std):\n",
    "r\"\"\"Reverses normalization of the torch Tensor.\n",
    "    \n",
    "    To show images of the databunch correctly, \n",
    "    the pytorch normalization has to be reversed using this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor_in : torch.Tensor\n",
    "        The normalized tensor\n",
    "    mean : array\n",
    "        The mean values used to create `tensor_in`\n",
    "    std : array\n",
    "        The standard deviation values to create `tensor_in`\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch Tensor\n",
    "        The reversed normalized tensor\n",
    "        \n",
    "    See Also\n",
    "    --------\n",
    "    torchvision.transforms.Normalize()\n",
    "    \n",
    "\"\"\"\n",
    "  std = torch.FloatTensor(std)\n",
    "  mean = torch.FloatTensor(mean)\n",
    "  tensor_out = torch.zeros([tensor_in.shape[0], tensor_in.shape[1], tensor_in.shape[2]], dtype=torch.float)\n",
    "  tensor_out[0] = tensor_in[0] * std[0] + mean[0]\n",
    "  tensor_out[1] = tensor_in[1] * std[1] + mean[1]\n",
    "  tensor_out[2] = tensor_in[2] * std[2] + mean[2]\n",
    "  return tensor_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lS19Wt2CmSDW"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "r\"\"\"Show an image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img : torch.Tensor\n",
    "        The normalized torch.Tensor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plot of the image\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    .numpy()\n",
    "        Converts torch.Tensor to numpy array\n",
    "    .transpose()\n",
    "        Transposes numpy array\n",
    "    \n",
    "\"\"\"\n",
    "  normalised = reverse_normalization(img, mean =[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  out = normalised.numpy().transpose((1, 2, 0))\n",
    "  plt.imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpDtGUoEmVNo"
   },
   "outputs": [],
   "source": [
    "def show_bunch(dataset, number=5, column='profile_icon'):\n",
    "r\"\"\"Show a number of random images from the dataset.\n",
    "    \n",
    "    Shows `number` of pictures from the `dataset`\n",
    "    `column` specifies the column of the correct label.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : ProfilesDataset\n",
    "        The dataset of which the images are shown\n",
    "    number : int\n",
    "        The number of images to show\n",
    "    column : string\n",
    "        The name of the column containing the label.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plot of the images\n",
    "        \n",
    "    See Also\n",
    "    --------\n",
    "    torch.utils.data.RandomSampler()\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    I used information from this stackoverflow question\n",
    "    https://stackoverflow.com/questions/11159436/multiple-figures-in-a-single-window\n",
    "    \n",
    "\"\"\"\n",
    "  bunch = torch.utils.data.RandomSampler(dataset, replacement=True, num_samples=number)\n",
    "  \n",
    "  rows = int(np.ceil(number / 5))\n",
    "  fig, axeslist = plt.subplots(ncols=5, nrows=rows, figsize=(20, 20))\n",
    "\n",
    "  for ind, data_index in enumerate(bunch):\n",
    "    image = dataset[data_index]['image']\n",
    "    normalised = reverse_normalization(image, mean =[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    img = normalised.numpy().transpose((1, 2, 0))\n",
    "    label = dataset[data_index][column]\n",
    "    axeslist.ravel()[ind].imshow(img)\n",
    "    axeslist.ravel()[ind].set_title(np.round(label.item(), 2))\n",
    "    axeslist.ravel()[ind].set_axis_off()\n",
    "\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CU4PV7qLou7j"
   },
   "outputs": [],
   "source": [
    "# Makes a new resnet18 model, which is not pretrained. The last linear layer is changed to ouput the same number of layers as classes.\n",
    "# Later the saved parameters are loaded in this model\n",
    "def new_model():\n",
    "r\"\"\"Creates a not pretrained resnet18 model.\n",
    "    \n",
    "    After creating the model, the last layer is changed to output\n",
    "    the number of classes we are interested in.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Pytorch resnet18 model\n",
    "        \n",
    "        \n",
    "    See Also\n",
    "    --------\n",
    "    torchvision.models.resnet18()\n",
    "    torch.nn.Sequential()\n",
    "    torch.nn.Linear()\n",
    "    \n",
    "\"\"\"    \n",
    "  model = torchvision.models.resnet18(pretrained=False)\n",
    "  # Make the last linear layer equal to the dimension of classes\n",
    "  class_number = len(classes)\n",
    "  lin = model.fc\n",
    "\n",
    "  new_lin = nn.Sequential(\n",
    "      nn.Linear(lin.in_features, class_number, bias=True)\n",
    "  )\n",
    "\n",
    "  model.fc = new_lin\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeRvkwYGT5G7"
   },
   "source": [
    "## Datasetclass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ycemXJ6zT3TZ"
   },
   "outputs": [],
   "source": [
    "class ProfilesDataset(torch.utils.data.Dataset):\n",
    "r\"\"\"A custom dataset for the race profiles.\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        This dataframe contains the information of the race profiles.\n",
    "    root_dir : string\n",
    "        String to the directory where the imgs/ folder is.\n",
    "    y_column : string\n",
    "        String containing the name of the column containing the label.\n",
    "    transform : transforms.Compose()\n",
    "        List of transform objects\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dataset\n",
    "        Dataset with each sample containing the idx in the dataframe,\n",
    "        the image converted to a tensor and the label.\n",
    "        \n",
    "    See Also\n",
    "    --------\n",
    "    torch.utils.data.Dataset\n",
    "\"\"\"\n",
    "  def __init__(self, df, root_dir, y_column, transform=None):\n",
    "    self.profiles_df = df\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "    self.y_column = y_column\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.profiles_df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if torch.is_tensor(idx):\n",
    "      idx = idx.tolist()\n",
    "\n",
    "    img_name = os.path.join(self.root_dir, self.profiles_df.iloc[idx]['img_name'])\n",
    "    image = Image.open(img_name)\n",
    "    profile_icon = self.profiles_df.iloc[idx][self.y_column]\n",
    "    df_idx = idx\n",
    "\n",
    "    sample = {'df_idx': df_idx, 'image': image, 'profile_icon': torch.tensor(profile_icon, dtype=torch.long)}\n",
    "\n",
    "    try:\n",
    "      if self.transform:\n",
    "        sample['image'] = self.transform(sample['image'])\n",
    "    except:\n",
    "      profile_race = self.profiles_df.iloc[idx]['race']\n",
    "      print(f'Deze geeft een error:{profile_race}')\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_g0D54JmgeD"
   },
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gsIFji04mfWX"
   },
   "outputs": [],
   "source": [
    "# Transforms that are used for the images. The images are resized, converted to a tensor and normalized.\n",
    "data_transforms = transforms.Compose([\n",
    "      transforms.Resize((256, 256)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmL_TroImnmD"
   },
   "source": [
    "## Load the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O0KE9vEMnLQt"
   },
   "outputs": [],
   "source": [
    "# unzip the imgs\n",
    "!unzip -o -q '/content/content/My Drive/TdF/Profiles/more_data/imgs.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Iyaw_8GyUeRe",
    "outputId": "26f44246-af4c-44dc-da07-88ade433e43f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the csv file which contains the data\n",
    "races = pd.read_csv('/content/content/My Drive/TdF/Profiles/races_to_classify.csv', delimiter=';')\n",
    "# Races to classify is every race without a profile_icon, which is p0\n",
    "races = races[races['profile_icon'] == 'p0']\n",
    "races.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "gm8ebBc5nHqP",
    "outputId": "1b17d144-4c16-4fa3-bad9-f421924de82d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot identify image file 'imgs/0race-tour-of-austria-2013-stage-3.jpg-large'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(135, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are some corrupt images that are not able to be transformed, therefore remove these\n",
    "test_transform = data_transforms\n",
    "ind_errors = []\n",
    "for ind, row in races.iterrows():\n",
    "  try:\n",
    "    im = Image.open(row['img_name'])\n",
    "    test = test_transform(im)\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    ind_errors.append(ind)\n",
    "\n",
    "races = races.drop(ind_errors)\n",
    "races.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O849PIKcsg-0"
   },
   "outputs": [],
   "source": [
    "#Change the class strings to numbers\n",
    "class_dict = {'p0': 0}\n",
    "races['profile_icon'] = races['profile_icon'].map(class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "2Uv3QbjTsoOn",
    "outputId": "055df2e5-6610-449e-c979-fce62d08ba48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0., 135.,   0.,   0.,   0.,   0.]),\n",
       " array([-0.5, -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if the current profile icons are all 0\n",
    "plt.hist(races['profile_icon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoN7AkQ8uyGD"
   },
   "outputs": [],
   "source": [
    "# Reset index\n",
    "races = races.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qB0EtNMxwE0U"
   },
   "outputs": [],
   "source": [
    "# Copy dataframe race\n",
    "pred_races = races.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9AKbNQdAog-W"
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P6CGFMuOoIZl"
   },
   "outputs": [],
   "source": [
    "# Create a new model and send it to GPU\n",
    "model = new_model()\n",
    "model.to(device)\n",
    "trained_model = '/content/content/My Drive/TdF/Profiles/more_data/profile_classifier.pt'\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load(trained_model))\n",
    "model.eval()\n",
    "pass # No output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWgHwnAypHG6"
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "predict_set = ProfilesDataset(df=races, root_dir='', y_column='profile_icon', transform=data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "7QU11pM7sZG0",
    "outputId": "1aafd810-5b23-4060-9c2b-7791ace354ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'df_idx': 0,\n",
       " 'image': tensor([[[2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          ...,\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489],\n",
       "          [2.2489, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2489]],\n",
       " \n",
       "         [[2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          ...,\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286],\n",
       "          [2.4286, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.4286]],\n",
       " \n",
       "         [[2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          ...,\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400],\n",
       "          [2.6400, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6400]]]),\n",
       " 'profile_icon': tensor(0)}"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "94uHdV4Ms3Cq"
   },
   "outputs": [],
   "source": [
    "# Load the data set in the loader\n",
    "predict_loader = torch.utils.data.DataLoader(predict_set, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WE17-Pnls_ih"
   },
   "outputs": [],
   "source": [
    "# Predict profile_icon and load in the dataframe\n",
    "for i, data in enumerate(predict_loader, 0):\n",
    "  input = data['image'].to(device)\n",
    "  df_idx = data['df_idx']\n",
    "\n",
    "  output = model(input)\n",
    "  _, predicted = torch.max(output.data, 1)\n",
    "  pred_races.loc[df_idx.item(), 'profile_icon'] = predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "colab_type": "code",
    "id": "A-0orfgHukwx",
    "outputId": "6c3182c1-ee39-4060-c6f5-af79526f5134"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>race</th>\n",
       "      <th>profile_score</th>\n",
       "      <th>point_scale</th>\n",
       "      <th>img_name</th>\n",
       "      <th>profile_icon</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>172</td>\n",
       "      <td>race/etoile-de-besseges/2020/stage-2</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.Stage</td>\n",
       "      <td>imgs/0race-etoile-de-besseges-2020-stage-2.jpg</td>\n",
       "      <td>2</td>\n",
       "      <td>Alés</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>race/etoile-de-besseges/2020/stage-3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.Stage</td>\n",
       "      <td>imgs/0race-etoile-de-besseges-2020-stage-3.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>Givors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>174</td>\n",
       "      <td>race/etoile-de-besseges/2020/stage-4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.Stage</td>\n",
       "      <td>imgs/0race-etoile-de-besseges-2020-stage-4.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4023</td>\n",
       "      <td>4023</td>\n",
       "      <td>race/tour-of-turkey/2015/stage-7</td>\n",
       "      <td>0</td>\n",
       "      <td>2.HC.Stage</td>\n",
       "      <td>imgs/0race-tour-of-turkey-2015-stage-7.png</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4025</td>\n",
       "      <td>4025</td>\n",
       "      <td>race/tour-de-yorkshire/2015/stage-1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.1.Stage</td>\n",
       "      <td>imgs/0race-tour-de-yorkshire-2015-stage-1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0  ...  profile_icon location\n",
       "0      0         172  ...             2     Alés\n",
       "1      1         173  ...             1   Givors\n",
       "2      2         174  ...             4      NaN\n",
       "3      3        4023  ...             0      NaN\n",
       "4      4        4025  ...             1      NaN\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_races.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "43ejh_6ZugBc",
    "outputId": "d957b0fa-7cfc-4e79-d326-ef5615a9c822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([30.,  0., 32.,  0.,  0., 14.,  0., 27.,  0., 32.]),\n",
       " array([0. , 0.4, 0.8, 1.2, 1.6, 2. , 2.4, 2.8, 3.2, 3.6, 4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram of the predicted profile_icons\n",
    "plt.hist(pred_races['profile_icon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwfNhR9ZwjiV"
   },
   "outputs": [],
   "source": [
    "# Save the predictions in a csv file\n",
    "pred_races.to_csv('/content/content/My Drive/TdF/Profiles/predicted_races.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "classify_profiles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
